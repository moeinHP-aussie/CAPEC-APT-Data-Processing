{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„ÛŒ Ø¨Ø®Ø´ Ø§ÙˆÙ„::\n",
        "Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ø³Ø§ÛŒØ¨Ø±ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ **CAPEC** Ùˆ **APT** Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
        "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ **CAPEC** Ø´Ø§Ù…Ù„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø­Ù…Ù„Ø§Øª Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¨Ù‡ Ø¹Ø¨Ø§Ø±Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù‡Ø± Ø­Ù…Ù„Ù‡ Ø§Ø´Ø§Ø±Ù‡ Ø¯Ø§Ø±Ù†Ø¯.\n",
        "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ **APT** Ø´Ø§Ù…Ù„ Ù†Ú¯Ø§Ø´Øª Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø¨Ù‡ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª (APT) Ø§Ø³Øª.\n",
        "Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ **SPLUNK** Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ (Natural Language Processing)\n",
        "Ù…Ø§Ù†Ù†Ø¯ **spaCy**ØŒ Ø­Ù…Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡ Ø±Ø§ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªØ·Ø¨ÛŒÙ‚ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
        "\n",
        "## Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ:\n",
        "1. **Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ CAPEC:** Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ CAPEC Ø§Ø² ÛŒÚ© ÙØ§ÛŒÙ„ CSV Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø´Ø§Ù…Ù„ Ù†Ø§Ù…ØŒ ØªÙˆØ¶ÛŒØ­Ø§ØªØŒ\n",
        "    Ùˆ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ø­Ù…Ù„Ø§Øª Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¹Ø¨Ø§Ø±Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "2. **Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT:** Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT Ø§Ø² ÛŒÚ© ÙØ§ÛŒÙ„ CSV Ø¯ÛŒÚ¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ù†Ú¯Ø§Ø´Øª Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡\n",
        "    Ù‡Ø± Ø§Ù„Ú¯ÙˆÛŒ CAPEC Ø¨Ù‡ Ú©Ø¯Ø§Ù… Ú¯Ø±ÙˆÙ‡ APT Ù…Ø±ØªØ¨Ø· Ø§Ø³Øª.\n",
        "3. **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ SPLUNK:** Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ SPLUNK Ú©Ù‡ Ø¯Ø± Ù‚Ø§Ù„Ø¨ ÛŒÚ© ÙØ§ÛŒÙ„ JSON Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
        "    Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ù‡Ø± Ù„Ø§Ú¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **spaCy PhraseMatcher** Ø¨Ø§ Ø¹Ø¨Ø§Ø±Ø§Øª CAPEC ØªØ·Ø¨ÛŒÙ‚ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "4. **ÙÛŒÙ„ØªØ± APT:** Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²ØŒ ØªÙ†Ù‡Ø§ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø´Ø§Ù…Ù„ APTÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ØŒ Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
        "5. **Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ³Ø· Ø®Ø¨Ø±Ù‡:** Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† Ø§Ù…Ù†ÛŒØªÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ Ø´Ø§Ù…Ù„\n",
        "    Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ØªÙ‡Ø¯ÛŒØ¯ (benign/Ù…Ø´Ú©ÙˆÚ©) Ø§Ø³Øª.\n",
        "\n",
        "## ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ:\n",
        "- **CAPEC_CSV:** ÙØ§ÛŒÙ„ CSV Ú©Ù‡ Ø´Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ CAPEC Ø§Ø³Øª.\n",
        "- **APT_CSV:** ÙØ§ÛŒÙ„ CSV Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT Ø§Ø³Øª.\n",
        "- **SPLUNK_JSON:** ÙØ§ÛŒÙ„ JSON Ú©Ù‡ Ø´Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ SPLUNK Ø§Ø³Øª.\n",
        "\n",
        "## ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ:\n",
        "- **OUTPUT_CSV:** ÙØ§ÛŒÙ„ CSV Ú©Ù‡ Ø´Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ø§Ø³ØªØŒ Ø´Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø®ØªÙ„Ù Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ù…Ø§Ù†Ù†Ø¯\n",
        "    Ø²Ù…Ø§Ù†ØŒ Ù…Ù†Ø¨Ø¹ØŒ IP Ú©Ø§Ø±Ø¨Ø±ØŒ Ùˆ ÙˆØ¶Ø¹ÛŒØª ØªÙ‡Ø¯ÛŒØ¯.\n",
        "- **EXPERT_REPORT_CSV:** ÙØ§ÛŒÙ„ CSV Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø´Ø§Ù…Ù„\n",
        "    Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ùˆ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ØµØ­Øª Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ ØªÙˆØ³Ø· Ø®Ø¨Ø±Ù‡ Ø§Ø³Øª.\n",
        "\n",
        "## ØªÙØ³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ:\n",
        "Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª ÛŒÚ© ÙØ§ÛŒÙ„ CSV Ø´Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ù‡Ø± Ø±Ú©ÙˆØ±Ø¯ Ø¢Ù† Ø´Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ú¯\n",
        "Ùˆ Ø¨Ø±Ú†Ø³Ø¨ ØªÙ‡Ø¯ÛŒØ¯ (benign/Ù…Ø´Ú©ÙˆÚ©) Ø§Ø³Øª.\n",
        "Ø¯Ø± Ø§ÛŒÙ† ÙØ§ÛŒÙ„ØŒ Ø¨Ø±Ú†Ø³Ø¨ \"benign\" Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø¹Ø¯Ù… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªÙ‡Ø¯ÛŒØ¯ Ø§Ø³Øª (1 = Ø¨ÛŒâ€ŒØ®Ø·Ø±)ØŒ Ùˆ Ø§Ú¯Ø± CAPECÙ‡Ø§ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø´ÙˆÙ†Ø¯ØŒ\n",
        "Ø¨Ø±Ú†Ø³Ø¨ \"benign\" Ø¨Ù‡ 0 ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
        "Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø± ÙØ§ÛŒÙ„ **EXPERT_REPORT_CSV**ØŒ Ú©Ø§Ø±Ø´Ù†Ø§Ø³ Ø§Ù…Ù†ÛŒØªÛŒ Ø¨Ø§ÛŒØ¯ ØµØ­Øª Ø§ÛŒÙ† Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†Ø¯.\n",
        "\n",
        "### Ú¯Ø²Ø§Ø±Ø´ ØªØ·Ø¨ÛŒÙ‚ APT:\n",
        "Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ø§Ú¯Ø± APTÙ‡Ø§ÛŒÛŒ Ø®Ø§Øµ Ø§Ù†ØªØ®Ø§Ø¨ Ø´ÙˆÙ†Ø¯ØŒ Ú¯Ø²Ø§Ø±Ø´ÛŒ Ø§Ø² ØªØ¹Ø¯Ø§Ø¯ Ø­Ù…Ù„Ø§Øª ØªØ·Ø¨ÛŒÙ‚ Ø¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Ø§ÛŒÙ† APTÙ‡Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "\n",
        "\n",
        "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø§Ù…Ú©Ø§Ù† ÙÛŒÙ„ØªØ± Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ\n",
        "Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± ØªÙˆØ³Ø· Ú©Ø§Ø±Ø´Ù†Ø§Ø³Ø§Ù† Ø§Ù…Ù†ÛŒØªÛŒ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "Ø¯Ø± Ù†Ù‡Ø§ÛŒØªØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆØ§ÙÙ‚ Ø¨ÛŒÙ† Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±Ù‡ (Cohen's Kappa)ØŒ Ú©ÛŒÙÛŒØª Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n"
      ],
      "metadata": {
        "id": "fsVWrwH0R9aP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1oxKK0VHdoZ",
        "outputId": "2b60aa92-2ab7-4efa-c579-6ebc7ad425ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² JSON: 209\n",
            "ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ: 209\n",
            "âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ Ø´Ø¯Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø®Ø·Ø§: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import spacy\n",
        "import logging\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ù„Ø§Ú¯ÛŒÙ†Ú¯\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† spaCy Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…ØªÙ†\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def normalize_text(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ø´Ø§Ù…Ù„ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©ØŒ Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ù†Ø§Ø®ÙˆØ§Ø³ØªÙ‡ØŒ Ù„ÙÙ…Ø§ØªÛŒØ²Ù‡ Ú©Ø±Ø¯Ù† Ùˆ Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù.\n",
        "    \"\"\"\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "    return tokens\n",
        "\n",
        "def load_capec_data(capec_csv: str) -> Tuple[Dict[str, List[str]], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ CAPEC Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ù„Ú¯Ùˆ.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(capec_csv)\n",
        "    capec_phrases = {}\n",
        "    capec_details = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        capec_id = f\"CAPEC-{row['ID']}\" if not str(row['ID']).startswith('CAPEC-') else row['ID']\n",
        "\n",
        "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¨Ø§Ø±Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·\n",
        "        phrases = []\n",
        "        for field in ['Name', 'Description', 'Example Instances', 'Alternate Terms']:\n",
        "            text = str(row.get(field, ''))\n",
        "            if text.lower() != 'nan' and text.strip():\n",
        "                # ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¹Ø¨Ø§Ø±Ø§Øª\n",
        "                raw_phrases = [phrase.strip() for phrase in re.split(r'[;,.]', text) if phrase.strip()]\n",
        "                for phrase in raw_phrases:\n",
        "                    # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¹Ø¨Ø§Ø±ØªØŒ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÛŒØ§Ø² ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯)\n",
        "                    phrases.append(phrase.lower())\n",
        "        # Ø­Ø°Ù Ø¹Ø¨Ø§Ø±Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ùˆ Ø¹Ø¨Ø§Ø±Ø§Øª Ú©ÙˆØªØ§Ù‡\n",
        "        phrases = list(set([p for p in phrases if len(p) > 3]))\n",
        "        capec_phrases[capec_id] = phrases\n",
        "        capec_details.append({\n",
        "            'CAPEC-ID': capec_id,\n",
        "            'Name': row['Name'],\n",
        "            'Severity': row.get('Typical Severity', 'Medium'),\n",
        "            'Description': row.get('Description', ''),\n",
        "            'Keywords': phrases\n",
        "        })\n",
        "\n",
        "    return capec_phrases, pd.DataFrame(capec_details)\n",
        "\n",
        "def load_apt_mapping(apt_csv: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT Ø§Ø² ÙØ§ÛŒÙ„ CSV.\n",
        "    ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ ÙØ§ÛŒÙ„ Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'CAPEC-ID' Ùˆ 'APT Group' Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.\n",
        "    \"\"\"\n",
        "    apt_df = pd.read_csv(apt_csv)\n",
        "    return apt_df\n",
        "\n",
        "def process_logs(splunk_data: List[Dict], capec_phrases: Dict, apt_mapping: pd.DataFrame,\n",
        "                 selected_apts: List[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ SPLUNK Ùˆ ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ø¹Ø¨Ø§Ø±Ø§Øª CAPEC Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² spaCy PhraseMatcher.\n",
        "    Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø¢Ù†ØŒ Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "    \"\"\"\n",
        "    print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² JSON: {len(splunk_data)}\")  # Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ ÙˆØ±ÙˆØ¯ÛŒ\n",
        "\n",
        "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "    patterns = {capec_id: [nlp(text) for text in phrases] for capec_id, phrases in capec_phrases.items()}\n",
        "\n",
        "    for capec_id, pattern_list in patterns.items():\n",
        "        matcher.add(capec_id, pattern_list)\n",
        "\n",
        "    processed_data = []\n",
        "    skipped_logs = 0\n",
        "\n",
        "    for idx, log in enumerate(splunk_data):\n",
        "        try:\n",
        "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø§Ø² Ù„Ø§Ú¯ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶\n",
        "            features = {\n",
        "                '_time': log.get('_time', ''),\n",
        "                'host': log.get('host', ''),\n",
        "                'source': log.get('source', ''),\n",
        "                'sourcetype': log.get('sourcetype', ''),\n",
        "                'status': log.get('status', 0),\n",
        "                'bytes': log.get('bytes', 0),\n",
        "                'clientip': log.get('clientip', ''),\n",
        "                'url': log.get('url', ''),\n",
        "                'level': log.get('level', ''),\n",
        "                'component': log.get('component', ''),\n",
        "                'message': log.get('message', '').lower() if log.get('message') else ''\n",
        "            }\n",
        "\n",
        "            # Ù¾Ø±Ø¯Ø§Ø²Ø´ CAPEC\n",
        "            doc = nlp(features['message'])\n",
        "            matches = matcher(doc)\n",
        "            matched_capecs = list(set([nlp.vocab.strings[match_id] for match_id, _, _ in matches]))\n",
        "            features['matched_capecs'] = ', '.join(matched_capecs)\n",
        "            features['benign'] = 1 if not matched_capecs else 0\n",
        "\n",
        "            # Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT\n",
        "            apt_list = []\n",
        "            for capec in matched_capecs:\n",
        "                apt_rows = apt_mapping[apt_mapping['CAPEC-ID'] == capec]\n",
        "                if not apt_rows.empty:\n",
        "                    apt_list.extend(apt_rows['APT Group'].tolist())\n",
        "            apt_list = list(set(apt_list))\n",
        "            features['matched_apts'] = ', '.join(apt_list)\n",
        "            features['expert_validation'] = None\n",
        "\n",
        "            processed_data.append(features)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù„Ø§Ú¯ Ø´Ù…Ø§Ø±Ù‡ {idx}: {e}\")\n",
        "            skipped_logs += 1\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(processed_data)\n",
        "    print(f\"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ: {len(df)}\")\n",
        "    print(f\"âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ Ø´Ø¯Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø®Ø·Ø§: {skipped_logs}\")\n",
        "\n",
        "    # -----------------------\n",
        "    # Ø§Ú¯Ø± APT Ø®Ø§ØµÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ ÙÛŒÙ„ØªØ± Ú©Ù†ÛŒÙ…\n",
        "    if selected_apts is not None and len(selected_apts) > 0:\n",
        "        def filter_apt(row):\n",
        "            if row['matched_apts']:\n",
        "                apt_set = set([apt.strip() for apt in row['matched_apts'].split(',')])\n",
        "                return bool(apt_set.intersection(set(selected_apts)))\n",
        "            return False\n",
        "\n",
        "        initial_count = len(df)\n",
        "        df = df[df.apply(filter_apt, axis=1)]\n",
        "        filtered_count = len(df)\n",
        "        print(f\"ğŸ“Š ÙÛŒÙ„ØªØ± Ø²ÛŒØ±Ù…Ø¬Ù…ÙˆØ¹Ù‡ APT: Ø§Ø² {initial_count} Ø±Ú©ÙˆØ±Ø¯ØŒ {filtered_count} Ø±Ú©ÙˆØ±Ø¯ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù†Ø¯.\")\n",
        "        # Ú¯Ø²Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ Ù‡Ø± APT Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡\n",
        "        apt_summary = {}\n",
        "        for apt in selected_apts:\n",
        "            count = df['matched_apts'].str.contains(apt, regex=False).sum()\n",
        "            apt_summary[apt] = count\n",
        "        print(\"ğŸ” Ú¯Ø²Ø§Ø±Ø´ Ø§Ù†ØªØ®Ø§Ø¨ Ø²ÛŒØ±Ù…Ø¬Ù…ÙˆØ¹Ù‡ APT:\", apt_summary)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def generate_expert_report(df: pd.DataFrame, output_file: str):\n",
        "    \"\"\"\n",
        "    Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ³Ø· Ø®Ø¨Ø±Ù‡.\n",
        "    \"\"\"\n",
        "    report = df[[\n",
        "        '_time', 'host', 'source', 'sourcetype', 'status', 'bytes',\n",
        "        'clientip', 'url', 'level', 'component', 'message', 'matched_capecs',\n",
        "        'matched_apts', 'benign'\n",
        "    ]].copy()\n",
        "    report['expert_capecs'] = ''\n",
        "    report['expert_benign'] = ''\n",
        "    report.to_csv(output_file, index=False)\n",
        "    logging.info(f\"Ú¯Ø²Ø§Ø±Ø´ Ø®Ø¨Ø±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "def calculate_irr(auto_labels, expert_labels):\n",
        "    \"\"\"Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙˆØ§ÙÙ‚ Ø¨ÛŒÙ† Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ùˆ Ø®Ø¨Ø±Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Cohen's Kappa.\"\"\"\n",
        "    return cohen_kappa_score(auto_labels, expert_labels)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§\n",
        "    CAPEC_CSV = 'capec_dataset.csv'         # ÙØ§ÛŒÙ„ CAPEC\n",
        "    APT_CSV = 'capec_apt_mapping.csv'         # ÙØ§ÛŒÙ„ Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT\n",
        "    SPLUNK_JSON = \"splunk_logs_sample.json\"   # ÙØ§ÛŒÙ„ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ SPLUNK\n",
        "    OUTPUT_CSV = 'labeled_threats.csv'\n",
        "    EXPERT_REPORT_CSV = 'expert_review.csv'\n",
        "\n",
        "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ CAPEC Ùˆ Ù†Ú¯Ø§Ø´Øª APT\n",
        "    capec_phrases, capec_df = load_capec_data(CAPEC_CSV)\n",
        "    apt_mapping = load_apt_mapping(APT_CSV)\n",
        "\n",
        "    with open(SPLUNK_JSON, 'r') as f:\n",
        "        splunk_data = json.load(f)\n",
        "\n",
        "    selected_apts = None  # ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù„ÛŒØ³Øª APTÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯\n",
        "    processed_df = process_logs(splunk_data, capec_phrases, apt_mapping, selected_apts)\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø§ÙˆÙ„ÛŒÙ‡\n",
        "    processed_df.to_csv(OUTPUT_CSV, index=False)\n",
        "    logging.info(f\"ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø¯Ø± {OUTPUT_CSV} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n",
        "\n",
        "    # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø¨Ø±Ù‡\n",
        "    generate_expert_report(processed_df, EXPERT_REPORT_CSV)\n",
        "    logging.info(f'âœ… Ù…Ø±Ø­Ù„Ù‡ Û± ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯! Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ {EXPERT_REPORT_CSV} Ø±Ø§ Ø¨Ù‡ Ø®Ø¨Ø±Ù‡ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯.')\n",
        "\n",
        "    # Ù¾Ø³ Ø§Ø² Ø¯Ø±ÛŒØ§ÙØª ÙØ§ÛŒÙ„ Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡ ØªÙˆØ³Ø· Ø®Ø¨Ø±Ù‡:\n",
        "    # expert_reviewed = pd.read_csv('expert_reviewed.csv')\n",
        "    # kappa = calculate_irr(processed_df['benign'], expert_reviewed['expert_benign'])\n",
        "    # logging.info(f'Ù…ÛŒØ²Ø§Ù† ØªÙˆØ§ÙÙ‚ (IRR): {kappa:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "<h2> Ù…Ø±Ø§Ø­Ù„ Ú©Ø§Ø± Ø¨Ø®Ø´ Ø¯ÙˆÙ… Ù¾Ø±ÙˆÚ˜Ù‡</h2>\n",
        "\n",
        "<h3>1ï¸âƒ£ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡</h3>\n",
        "<ul>\n",
        "  <li>ÙˆØ±ÙˆØ¯ÛŒ: ÙØ§ÛŒÙ„ <code>labeled_threats.csv</code> (Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„)</li>\n",
        "  <li>Ø´Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ùˆ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ: <code>matched_capecs</code>ØŒ <code>matched_apts</code>ØŒ <code>benign</code>ØŒ <code>expert_validation</code></li>\n",
        "  <li>Ø¨Ø±Ú†Ø³Ø¨ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø³ØªÙˆÙ† <code>final_label</code> Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯</li>\n",
        "</ul>\n",
        "\n",
        "<h3>2ï¸âƒ£ ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª</h3>\n",
        "<ul>\n",
        "  <li>Ø¨Ø§ <code>train_test_split</code> Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯Ùˆ Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´ (70Ùª) Ùˆ ØªØ³Øª (30Ùª) ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯</li>\n",
        "  <li>Ø§Ø² <code>stratify</code> Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ù†Ø³Ø¨Øª ØªÙ‡Ø¯ÛŒØ¯ Ùˆ Ø¹Ø§Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯</li>\n",
        "</ul>\n",
        "\n",
        "<h3>3ï¸âƒ£ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§</h3>\n",
        "<ul>\n",
        "  <li><strong>ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ØªÙ†ÛŒ:</strong> Ø³ØªÙˆÙ† <code>message</code> â†’ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ <code>TF-IDF</code> Ùˆ n-gram</li>\n",
        "  <li><strong>ÙˆÛŒÚ˜Ú¯ÛŒ Ø¹Ø¯Ø¯ÛŒ:</strong> Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ <code>status</code>ØŒ <code>bytes</code> â†’ Ù…Ù‚ÛŒØ§Ø³â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ <code>RobustScaler</code></li>\n",
        "  <li><strong>ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ:</strong> Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ <code>level</code>ØŒ <code>component</code>ØŒ <code>host</code> â†’ Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ <code>OneHotEncoder</code></li>\n",
        "</ul>\n",
        "\n",
        "<h3>4ï¸âƒ£ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ</h3>\n",
        "<ul>\n",
        "  <li>Ù…Ø¯Ù„: <code>RandomForestClassifier</code> (Ù…Ø¯Ù„ Ù†Ø¸Ø§Ø±Øª Ø´Ø¯Ù‡)</li>\n",
        "  <li>ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø§ <code>GridSearchCV</code> Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± <code>n_estimators</code> Ùˆ <code>max_depth</code></li>\n",
        "</ul>\n",
        "\n",
        "<h3>5ï¸âƒ£ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„</h3>\n",
        "<ul>\n",
        "  <li>Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ ØªØ³Øª Ø¨Ø§ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ:</li>\n",
        "  <ul>\n",
        "    <li><code>classification_report</code>: precision, recall, f1-score</li>\n",
        "    <li><code>confusion_matrix</code>: ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ø±Ø³Øª Ùˆ ØºÙ„Ø·</li>\n",
        "    <li><strong>Recall Ø¨Ø±Ø§ÛŒ ØªÙ‡Ø¯ÛŒØ¯Ù‡Ø§</strong>: Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±</li>\n",
        "  </ul>\n",
        "</ul>\n",
        "\n",
        "<h3>6ï¸âƒ£ Ú¯Ø²Ø§Ø±Ø´ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ (APT)</h3>\n",
        "<ul>\n",
        "  <li>Ø¨Ø±Ø±Ø³ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ØªÙ‡Ø¯ÛŒØ¯ (label=0)</li>\n",
        "  <li>ØªØ­Ù„ÛŒÙ„ Ø³ØªÙˆÙ† <code>matched_capecs</code> Ùˆ Ù†Ú¯Ø§Ø´Øª Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ APT GroupÙ‡Ø§</li>\n",
        "  <li>Ù†Ù…Ø§ÛŒØ´ Ù†ÙˆØ¹ Ùˆ ØªØ¹Ø¯Ø§Ø¯ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª</li>\n",
        "</ul>\n",
        "\n",
        "<h3>7ï¸âƒ£ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡</h3>\n",
        "<ul>\n",
        "  <li>Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ <code>apt_aware_supervised.pkl</code> Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯</li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<h2>ğŸŸ¢ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§</h2>\n",
        "<ul>\n",
        "  <li><code>labeled_threats.csv</code>: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ®ÙˆØ±Ø¯Ù‡</li>\n",
        "  <li><code>capec_apt_mapping.csv</code>: Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT Group</li>\n",
        "</ul>\n",
        "\n",
        "<h2>ğŸ”µ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§</h2>\n",
        "<ul>\n",
        "  <li>Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ (precision, recall, confusion matrix)</li>\n",
        "  <li>Ú¯Ø²Ø§Ø±Ø´ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ APT</li>\n",
        "  <li>ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡: <code>apt_aware_supervised.pkl</code></li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<h2>ğŸ“„ Ù†Ø­ÙˆÙ‡ Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ ØªÙØ³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§</h2>\n",
        "\n",
        "<h3>1ï¸âƒ£ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡</h3>\n",
        "<p>Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ <code>joblib</code> Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯:</p>\n",
        "<pre><code>import joblib\n",
        "model = joblib.load('apt_aware_supervised.pkl')</code></pre>\n",
        "<p>Ø³Ù¾Ø³ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ù…Ø´Ø§Ø¨Ù‡ Ù‚Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†ÛŒØ¯.</p>\n",
        "\n",
        "<h3>2ï¸âƒ£ Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„</h3>\n",
        "<p>Ø´Ø§Ù…Ù„:</p>\n",
        "<ul>\n",
        "  <li><strong>Support:</strong> ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³</li>\n",
        "  <li><strong>Precision:</strong> Ø¯Ù‚Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§</li>\n",
        "  <li><strong>Recall:</strong> Ù…Ø®ØµÙˆØµØ§Ù‹ Ø¨Ø±Ø§ÛŒ ØªÙ‡Ø¯ÛŒØ¯Ù‡Ø§ Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù… Ø§Ø³Øª</li>\n",
        "  <li><strong>Confusion Matrix:</strong> Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ø±Ø³Øª Ùˆ ØºÙ„Ø·</li>\n",
        "</ul>\n",
        "\n",
        "<h3>3ï¸âƒ£ Ú¯Ø²Ø§Ø±Ø´ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ (APT)</h3>\n",
        "<p>Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ù…Ø¯Ù„ Ú†Ù‡ Ù†ÙˆØ¹ ØªÙ‡Ø¯ÛŒØ¯Ø§ØªÛŒ (APT GroupÙ‡Ø§) Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ú†Ù†Ø¯ Ù…ÙˆØ±Ø¯ Ø§Ø² Ù‡Ø± Ù†ÙˆØ¹ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡.</p>\n"
      ],
      "metadata": {
        "id": "Y53ThlaOQlAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, recall_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import joblib\n",
        "import logging\n",
        "\n",
        "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# ---------------------------\n",
        "def load_data(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø±Ú†Ø³Ø¨ Ù†Ù‡Ø§ÛŒÛŒ\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if 'expert_benign' in df.columns:\n",
        "        logging.info(\"âœ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±Ù‡\")\n",
        "        df['final_label'] = np.where(\n",
        "            df['expert_benign'].notna(),\n",
        "            df['expert_benign'].astype(int),\n",
        "            df['benign']\n",
        "        )\n",
        "    else:\n",
        "        logging.warning(\"âš ï¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±\")\n",
        "        df['final_label'] = df['benign']\n",
        "\n",
        "    logging.info(\"\\nğŸ“Š Ø¢Ù…Ø§Ø± Ú©Ù„ Ø¯Ø§Ø¯Ù‡:\")\n",
        "    logging.info(df['final_label'].value_counts())\n",
        "    return df\n",
        "\n",
        "# ---------------------------\n",
        "def get_preprocessor():\n",
        "    \"\"\"\n",
        "    Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø´Ø§Ù…Ù„ Ø¨Ø®Ø´ Ù…ØªÙ†ÛŒ (Ø¨Ø§ TF-IDF) Ùˆ Ø¹Ø¯Ø¯ÛŒ/Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ.\n",
        "    \"\"\"\n",
        "    text_processor = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(\n",
        "            max_features=3000,\n",
        "            ngram_range=(1, 2),\n",
        "            stop_words='english',\n",
        "            sublinear_tf=True,\n",
        "            analyzer='word'\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('text', text_processor, 'message'),\n",
        "            ('num', RobustScaler(), ['status', 'bytes']),\n",
        "            ('cat', OneHotEncoder(\n",
        "                handle_unknown='infrequent_if_exist',\n",
        "                sparse_output=False,\n",
        "                min_frequency=10\n",
        "            ), ['level', 'component', 'host'])\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "# ---------------------------\n",
        "def train_model_supervised(X_train: pd.DataFrame, y_train: pd.Series) -> Pipeline:\n",
        "    \"\"\"Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù†Ø¸Ø§Ø±Øª Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GridSearchCV Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§\"\"\"\n",
        "    preprocessor = get_preprocessor()\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    param_grid = {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [None, 10, 20],\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring='f1_weighted',\n",
        "        verbose=1,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    logging.info(\"\\nğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù†Ø¸Ø§Ø±Øª Ø´Ø¯Ù‡ Ø¨Ø§ GridSearchCV:\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    logging.info(f\"Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {grid_search.best_params_}\")\n",
        "\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# ---------------------------\n",
        "def analyze_apts(threat_data: pd.DataFrame, capec_apt_map: pd.DataFrame) -> dict:\n",
        "    \"\"\"ØªØ­Ù„ÛŒÙ„ APT Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ú¯Ø§Ø´Øª CAPEC Ø¨Ù‡ APT\"\"\"\n",
        "    apt_counts = {}\n",
        "    for _, row in threat_data.iterrows():\n",
        "        if pd.isna(row.get('matched_capecs')):\n",
        "            continue\n",
        "        capecs = [c.strip() for c in row['matched_capecs'].split(',')]\n",
        "        for capec in capecs:\n",
        "            apts = capec_apt_map[capec_apt_map['CAPEC-ID'] == capec]['APT Group']\n",
        "            for apt in apts:\n",
        "                apt_counts[apt] = apt_counts.get(apt, 0) + 1\n",
        "    return dict(sorted(apt_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# ---------------------------\n",
        "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.Series, capec_apt_map: pd.DataFrame):\n",
        "    \"\"\"Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ù†Ø¸Ø§Ø±Øª Ø´Ø¯Ù‡ Ùˆ Ú†Ø§Ù¾ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"\\nğŸ“Š Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª:\")\n",
        "    print(y_test.value_counts())\n",
        "\n",
        "    report = classification_report(\n",
        "        y_test,\n",
        "        y_pred,\n",
        "        target_names=['ØªÙ‡Ø¯ÛŒØ¯ (0)', 'Ø¹Ø§Ø¯ÛŒ (1)'],\n",
        "        digits=4\n",
        "    )\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    threat_recall = recall_score(y_test, y_pred, pos_label=0)\n",
        "\n",
        "    print(\"\\nğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯:\")\n",
        "    print(report)\n",
        "    print(\"\\nğŸ¯ Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ:\")\n",
        "    print(cm)\n",
        "    print(f\"\\nğŸ“ˆ Recall Ø¨Ø±Ø§ÛŒ ØªÙ‡Ø¯ÛŒØ¯Ù‡Ø§: {threat_recall:.2%}\")\n",
        "\n",
        "    # Ú¯Ø²Ø§Ø±Ø´ APT (Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø³ØªÙˆÙ† matched_capecs Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)\n",
        "    if 'matched_capecs' in X_test.columns:\n",
        "        X_test = X_test.copy()\n",
        "        X_test['predicted_label'] = y_pred\n",
        "        threat_logs = X_test[X_test['predicted_label'] == 0]\n",
        "        print(\"\\nğŸ” Ú¯Ø²Ø§Ø±Ø´ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ (APT) Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø±Ù†Ø§Ú©:\")\n",
        "        if not threat_logs.empty:\n",
        "            apt_report = analyze_apts(threat_logs, capec_apt_map)\n",
        "            for apt, count in apt_report.items():\n",
        "                print(f\"- {apt}: {count} Ù…ÙˆØ±Ø¯\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Ù‡ÛŒÚ† ØªÙ‡Ø¯ÛŒØ¯ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯!\")\n",
        "    else:\n",
        "        print(\"Ø³ØªÙˆÙ† matched_capecs Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³ØªØ› Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ APT ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.\")\n",
        "\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    INPUT_DATA = \"labeled_threats.csv\"\n",
        "    CAPEC_APT_MAP = \"capec_apt_mapping.csv\"\n",
        "\n",
        "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "    df = load_data(INPUT_DATA)\n",
        "\n",
        "    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª (70/30)\n",
        "    X = df.drop(columns=['final_label'])\n",
        "    y = df['final_label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(\"\\nğŸ“Š Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:\")\n",
        "    print(y_train.value_counts())\n",
        "    print(\"â–â–â–\")\n",
        "    print(\"ğŸ“Š Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª:\")\n",
        "    print(y_test.value_counts())\n",
        "    print(\"â–â–â–\")\n",
        "\n",
        "    capec_apt_map = pd.read_csv(CAPEC_APT_MAP)\n",
        "\n",
        "    # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
        "    model = train_model_supervised(X_train, y_train)\n",
        "\n",
        "    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„\n",
        "    evaluate_model(model, X_test, y_test, capec_apt_map)\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„\n",
        "    joblib.dump(model, 'apt_aware_supervised.pkl')\n",
        "    print(\"\\nğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYk74VtxHp9d",
        "outputId": "cee921f0-d880-4334-a33d-2000fb60cfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:âš ï¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:\n",
            "final_label\n",
            "1    85\n",
            "0    61\n",
            "Name: count, dtype: int64\n",
            "â–â–â–\n",
            "ğŸ“Š Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª:\n",
            "final_label\n",
            "1    36\n",
            "0    27\n",
            "Name: count, dtype: int64\n",
            "â–â–â–\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "\n",
            "ğŸ“Š Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª:\n",
            "final_label\n",
            "1    36\n",
            "0    27\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   ØªÙ‡Ø¯ÛŒØ¯ (0)     1.0000    1.0000    1.0000        27\n",
            "    Ø¹Ø§Ø¯ÛŒ (1)     1.0000    1.0000    1.0000        36\n",
            "\n",
            "    accuracy                         1.0000        63\n",
            "   macro avg     1.0000    1.0000    1.0000        63\n",
            "weighted avg     1.0000    1.0000    1.0000        63\n",
            "\n",
            "\n",
            "ğŸ¯ Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ:\n",
            "[[27  0]\n",
            " [ 0 36]]\n",
            "\n",
            "ğŸ“ˆ Recall Ø¨Ø±Ø§ÛŒ ØªÙ‡Ø¯ÛŒØ¯Ù‡Ø§: 100.00%\n",
            "\n",
            "ğŸ” Ú¯Ø²Ø§Ø±Ø´ ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ (APT) Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø±Ù†Ø§Ú©:\n",
            "- China-based APT groups: 2 Ù…ÙˆØ±Ø¯\n",
            "- Chimera, APT-C-23, APT3: 1 Ù…ÙˆØ±Ø¯\n",
            "\n",
            "ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<div dir=\"rtl\">\n",
        "<h2> Ù…Ø¯Ù„ ØªÙ…Ø§Ù… ØªÙ‡Ø¯ÛŒØ¯Ø§Øª Ø±Ùˆ Ø¯Ø±Ø³Øª Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒ Ú©Ø±Ø¯Ù‡ Ú©Ù‡ Ø¨Ù‡ Ø¯Ù„Ø§ÛŒÙ„ Ø²ÛŒØ± Ù‡Ø³Øª:</h2>\n",
        "ØªØ¹Ø¯Ø§Ø¯ Ú©Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ ÛŒÚ©Ù†ÙˆØ§Ø®Øª Ø¨ÙˆØ¯Ù†:\n",
        "ÙÙ‚Ø· 209 Ù„Ø§Ú¯ Ø¯Ø§Ø±ÛŒÙ….\n",
        "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø®ÛŒÙ„ÛŒ Ù…ØªÙ†ÙˆØ¹ Ù†ÛŒØ³ØªÙ† Ùˆ Ø§Ù„Ú¯ÙˆÛŒ Ø®Ø§ØµÛŒ Ø¨Ù‡ ÙˆØ¶ÙˆØ­ Ø¨ÛŒÙ† ØªÙ‡Ø¯ÛŒØ¯ Ùˆ Ø¹Ø§Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡.\n",
        "\n",
        "Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø§ stratify split Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ‡Ø¯ÛŒØ¯ Ùˆ Ø¹Ø§Ø¯ÛŒ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª Ø­ÙØ¸ Ø´Ø¯Ù‡ Ùˆ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ Ø®ÛŒÙ„ÛŒ ÙˆØ§Ø¶Ø­ Ù‡Ø³ØªÙ†Ø¯.\n",
        "\n",
        "4ï¸âƒ£ Ù…Ø¯Ù„ RandomForest Ù‚Ø¯Ø±Øª Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¯Ø± Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ø¯Ø§Ø±Ù‡:\n",
        "RandomForest Ù…Ø¯Ù„ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ÛŒÙ‡ØŒ ÙˆÙ‚ØªÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙˆØ§Ø¶Ø­ Ùˆ Ø¨Ø¯ÙˆÙ† Ù†ÙˆÛŒØ² Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†ØŒ Ø¨Ù‡ Ø±Ø§Ø­ØªÛŒ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ù‡.\n",
        "\n"
      ],
      "metadata": {
        "id": "9tFAsZYTRLSM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbgsYx0TMs-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}