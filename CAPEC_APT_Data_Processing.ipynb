{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## عملکرد کلی بخش اول::\n",
        "این برنامه برای شناسایی تهدیدات سایبری با استفاده از داده‌های **CAPEC** و **APT** طراحی شده است.\n",
        "داده‌های **CAPEC** شامل الگوهای حملات هستند که به عبارات کلیدی و توضیحات مرتبط با هر حمله اشاره دارند.\n",
        "داده‌های **APT** شامل نگاشت این الگوها به گروه‌های تهدیدات (APT) است.\n",
        "برنامه لاگ‌های **SPLUNK** را پردازش می‌کند و با استفاده از تکنیک‌های پردازش زبان طبیعی (Natural Language Processing)\n",
        "مانند **spaCy**، حملات مشابه را شبیه‌سازی و تطبیق می‌دهد.\n",
        "\n",
        "## روش‌های پیاده‌سازی:\n",
        "1. **بارگذاری داده‌های CAPEC:** داده‌های CAPEC از یک فایل CSV بارگذاری می‌شود. این داده‌ها شامل نام، توضیحات،\n",
        "    و نمونه‌هایی از حملات هستند که عبارات کلیدی آن‌ها استخراج و ذخیره می‌شود.\n",
        "2. **بارگذاری نگاشت CAPEC به APT:** نگاشت CAPEC به APT از یک فایل CSV دیگر بارگذاری می‌شود. این نگاشت مشخص می‌کند که\n",
        "    هر الگوی CAPEC به کدام گروه APT مرتبط است.\n",
        "3. **پردازش لاگ‌های SPLUNK:** داده‌های SPLUNK که در قالب یک فایل JSON ذخیره شده‌اند، پردازش می‌شوند.\n",
        "    در این مرحله، هر لاگ با استفاده از **spaCy PhraseMatcher** با عبارات CAPEC تطبیق داده می‌شود.\n",
        "4. **فیلتر APT:** در صورت نیاز، تنها لاگ‌هایی که شامل APTهای انتخاب شده باشند، نگه داشته می‌شوند.\n",
        "5. **گزارش‌دهی و بررسی توسط خبره:** گزارش‌هایی برای بررسی توسط کارشناسان امنیتی ایجاد می‌شود. این گزارش شامل\n",
        "    اطلاعات لاگ‌های پردازش‌شده و برچسب‌های تهدید (benign/مشکوک) است.\n",
        "\n",
        "## فایل‌های ورودی:\n",
        "- **CAPEC_CSV:** فایل CSV که شامل داده‌های CAPEC است.\n",
        "- **APT_CSV:** فایل CSV که شامل نگاشت CAPEC به APT است.\n",
        "- **SPLUNK_JSON:** فایل JSON که شامل داده‌های لاگ‌های SPLUNK است.\n",
        "\n",
        "## فایل‌های خروجی:\n",
        "- **OUTPUT_CSV:** فایل CSV که شامل داده‌های پردازش‌شده است، شامل اطلاعات مختلف لاگ‌ها مانند\n",
        "    زمان، منبع، IP کاربر، و وضعیت تهدید.\n",
        "- **EXPERT_REPORT_CSV:** فایل CSV که برای بررسی توسط کارشناسان ارسال می‌شود. این فایل شامل\n",
        "    اطلاعات لاگ‌ها و ستون‌هایی برای بررسی صحت برچسب‌ها توسط خبره است.\n",
        "\n",
        "## تفسیر خروجی:\n",
        "پس از پردازش داده‌ها، در نهایت یک فایل CSV شامل داده‌های پردازش‌شده تولید می‌شود که هر رکورد آن شامل اطلاعات لاگ\n",
        "و برچسب تهدید (benign/مشکوک) است.\n",
        "در این فایل، برچسب \"benign\" نشان‌دهنده عدم شناسایی تهدید است (1 = بی‌خطر)، و اگر CAPECها شبیه‌سازی شوند،\n",
        "برچسب \"benign\" به 0 تغییر می‌کند.\n",
        "همچنین در فایل **EXPERT_REPORT_CSV**، کارشناس امنیتی باید صحت این برچسب‌ها را بررسی کند.\n",
        "\n",
        "### گزارش تطبیق APT:\n",
        "پس از پردازش، اگر APTهایی خاص انتخاب شوند، گزارشی از تعداد حملات تطبیق داده‌شده با این APTها تولید می‌شود.\n",
        "\n",
        "\n",
        "همچنین، امکان فیلتر و گزارش‌دهی\n",
        "برای بررسی دقیق‌تر توسط کارشناسان امنیتی فراهم می‌شود.\n",
        "در نهایت، با استفاده از توافق بین برچسب‌های خودکار و برچسب‌های خبره (Cohen's Kappa)، کیفیت برچسب‌ها ارزیابی می‌شود.\n"
      ],
      "metadata": {
        "id": "fsVWrwH0R9aP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1oxKK0VHdoZ",
        "outputId": "2b60aa92-2ab7-4efa-c579-6ebc7ad425ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تعداد لاگ‌های ورودی از JSON: 209\n",
            "📊 تعداد لاگ‌های پردازش شده نهایی: 209\n",
            "⚠️ تعداد لاگ‌های رد شده به دلیل خطا: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import spacy\n",
        "import logging\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# تنظیمات اولیه لاگینگ\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# بارگذاری مدل زبان spaCy برای پردازش پیشرفته متن\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def normalize_text(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    نرمال‌سازی متن شامل تبدیل به حروف کوچک، حذف کاراکترهای ناخواسته، لِماتیزه کردن و حذف کلمات توقف.\n",
        "    \"\"\"\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "    return tokens\n",
        "\n",
        "def load_capec_data(capec_csv: str) -> Tuple[Dict[str, List[str]], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    بارگذاری داده‌های CAPEC و ایجاد لیستی از عبارات کلیدی برای هر الگو.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(capec_csv)\n",
        "    capec_phrases = {}\n",
        "    capec_details = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        capec_id = f\"CAPEC-{row['ID']}\" if not str(row['ID']).startswith('CAPEC-') else row['ID']\n",
        "\n",
        "        # استخراج عبارات کلیدی از فیلدهای مرتبط\n",
        "        phrases = []\n",
        "        for field in ['Name', 'Description', 'Example Instances', 'Alternate Terms']:\n",
        "            text = str(row.get(field, ''))\n",
        "            if text.lower() != 'nan' and text.strip():\n",
        "                # تقسیم متن و نرمال‌سازی عبارات\n",
        "                raw_phrases = [phrase.strip() for phrase in re.split(r'[;,.]', text) if phrase.strip()]\n",
        "                for phrase in raw_phrases:\n",
        "                    # برای هر عبارت، نسخه‌های نرمال‌شده را استخراج می‌کنیم (می‌توان این بخش را بر اساس نیاز تغییر داد)\n",
        "                    phrases.append(phrase.lower())\n",
        "        # حذف عبارات تکراری و عبارات کوتاه\n",
        "        phrases = list(set([p for p in phrases if len(p) > 3]))\n",
        "        capec_phrases[capec_id] = phrases\n",
        "        capec_details.append({\n",
        "            'CAPEC-ID': capec_id,\n",
        "            'Name': row['Name'],\n",
        "            'Severity': row.get('Typical Severity', 'Medium'),\n",
        "            'Description': row.get('Description', ''),\n",
        "            'Keywords': phrases\n",
        "        })\n",
        "\n",
        "    return capec_phrases, pd.DataFrame(capec_details)\n",
        "\n",
        "def load_apt_mapping(apt_csv: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    بارگذاری نگاشت CAPEC به APT از فایل CSV.\n",
        "    فرض بر این است که فایل شامل ستون‌های 'CAPEC-ID' و 'APT Group' می‌باشد.\n",
        "    \"\"\"\n",
        "    apt_df = pd.read_csv(apt_csv)\n",
        "    return apt_df\n",
        "\n",
        "def process_logs(splunk_data: List[Dict], capec_phrases: Dict, apt_mapping: pd.DataFrame,\n",
        "                 selected_apts: List[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    پردازش لاگ‌های SPLUNK و تطبیق با عبارات CAPEC با استفاده از spaCy PhraseMatcher.\n",
        "    علاوه بر آن، نگاشت CAPEC به APT انجام می‌شود.\n",
        "    \"\"\"\n",
        "    print(f\"✅ تعداد لاگ‌های ورودی از JSON: {len(splunk_data)}\")  # مرحله اول بررسی تعداد ورودی\n",
        "\n",
        "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "    patterns = {capec_id: [nlp(text) for text in phrases] for capec_id, phrases in capec_phrases.items()}\n",
        "\n",
        "    for capec_id, pattern_list in patterns.items():\n",
        "        matcher.add(capec_id, pattern_list)\n",
        "\n",
        "    processed_data = []\n",
        "    skipped_logs = 0\n",
        "\n",
        "    for idx, log in enumerate(splunk_data):\n",
        "        try:\n",
        "            # استخراج فیلدهای مرتبط از لاگ با مقدار پیش‌فرض\n",
        "            features = {\n",
        "                '_time': log.get('_time', ''),\n",
        "                'host': log.get('host', ''),\n",
        "                'source': log.get('source', ''),\n",
        "                'sourcetype': log.get('sourcetype', ''),\n",
        "                'status': log.get('status', 0),\n",
        "                'bytes': log.get('bytes', 0),\n",
        "                'clientip': log.get('clientip', ''),\n",
        "                'url': log.get('url', ''),\n",
        "                'level': log.get('level', ''),\n",
        "                'component': log.get('component', ''),\n",
        "                'message': log.get('message', '').lower() if log.get('message') else ''\n",
        "            }\n",
        "\n",
        "            # پردازش CAPEC\n",
        "            doc = nlp(features['message'])\n",
        "            matches = matcher(doc)\n",
        "            matched_capecs = list(set([nlp.vocab.strings[match_id] for match_id, _, _ in matches]))\n",
        "            features['matched_capecs'] = ', '.join(matched_capecs)\n",
        "            features['benign'] = 1 if not matched_capecs else 0\n",
        "\n",
        "            # نگاشت CAPEC به APT\n",
        "            apt_list = []\n",
        "            for capec in matched_capecs:\n",
        "                apt_rows = apt_mapping[apt_mapping['CAPEC-ID'] == capec]\n",
        "                if not apt_rows.empty:\n",
        "                    apt_list.extend(apt_rows['APT Group'].tolist())\n",
        "            apt_list = list(set(apt_list))\n",
        "            features['matched_apts'] = ', '.join(apt_list)\n",
        "            features['expert_validation'] = None\n",
        "\n",
        "            processed_data.append(features)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ خطا در پردازش لاگ شماره {idx}: {e}\")\n",
        "            skipped_logs += 1\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(processed_data)\n",
        "    print(f\"📊 تعداد لاگ‌های پردازش شده نهایی: {len(df)}\")\n",
        "    print(f\"⚠️ تعداد لاگ‌های رد شده به دلیل خطا: {skipped_logs}\")\n",
        "\n",
        "    # -----------------------\n",
        "    # اگر APT خاصی انتخاب شده باشد، فیلتر کنیم\n",
        "    if selected_apts is not None and len(selected_apts) > 0:\n",
        "        def filter_apt(row):\n",
        "            if row['matched_apts']:\n",
        "                apt_set = set([apt.strip() for apt in row['matched_apts'].split(',')])\n",
        "                return bool(apt_set.intersection(set(selected_apts)))\n",
        "            return False\n",
        "\n",
        "        initial_count = len(df)\n",
        "        df = df[df.apply(filter_apt, axis=1)]\n",
        "        filtered_count = len(df)\n",
        "        print(f\"📊 فیلتر زیرمجموعه APT: از {initial_count} رکورد، {filtered_count} رکورد انتخاب شدند.\")\n",
        "        # گزارش تعداد هر APT انتخاب شده\n",
        "        apt_summary = {}\n",
        "        for apt in selected_apts:\n",
        "            count = df['matched_apts'].str.contains(apt, regex=False).sum()\n",
        "            apt_summary[apt] = count\n",
        "        print(\"🔍 گزارش انتخاب زیرمجموعه APT:\", apt_summary)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def generate_expert_report(df: pd.DataFrame, output_file: str):\n",
        "    \"\"\"\n",
        "    ایجاد گزارش برای بررسی توسط خبره.\n",
        "    \"\"\"\n",
        "    report = df[[\n",
        "        '_time', 'host', 'source', 'sourcetype', 'status', 'bytes',\n",
        "        'clientip', 'url', 'level', 'component', 'message', 'matched_capecs',\n",
        "        'matched_apts', 'benign'\n",
        "    ]].copy()\n",
        "    report['expert_capecs'] = ''\n",
        "    report['expert_benign'] = ''\n",
        "    report.to_csv(output_file, index=False)\n",
        "    logging.info(f\"گزارش خبره در فایل {output_file} ذخیره شد.\")\n",
        "\n",
        "def calculate_irr(auto_labels, expert_labels):\n",
        "    \"\"\"محاسبه توافق بین برچسب‌های خودکار و خبره با استفاده از Cohen's Kappa.\"\"\"\n",
        "    return cohen_kappa_score(auto_labels, expert_labels)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # پارامترها\n",
        "    CAPEC_CSV = 'capec_dataset.csv'         # فایل CAPEC\n",
        "    APT_CSV = 'capec_apt_mapping.csv'         # فایل نگاشت CAPEC به APT\n",
        "    SPLUNK_JSON = \"splunk_logs_sample.json\"   # فایل لاگ‌های SPLUNK\n",
        "    OUTPUT_CSV = 'labeled_threats.csv'\n",
        "    EXPERT_REPORT_CSV = 'expert_review.csv'\n",
        "\n",
        "    # بارگذاری داده‌های CAPEC و نگاشت APT\n",
        "    capec_phrases, capec_df = load_capec_data(CAPEC_CSV)\n",
        "    apt_mapping = load_apt_mapping(APT_CSV)\n",
        "\n",
        "    with open(SPLUNK_JSON, 'r') as f:\n",
        "        splunk_data = json.load(f)\n",
        "\n",
        "    selected_apts = None  # یا می‌توانید لیست APTهای مورد نظر را وارد کنید\n",
        "    processed_df = process_logs(splunk_data, capec_phrases, apt_mapping, selected_apts)\n",
        "\n",
        "    # ذخیره دیتاست اولیه\n",
        "    processed_df.to_csv(OUTPUT_CSV, index=False)\n",
        "    logging.info(f\"فایل خروجی برچسب‌گذاری شده در {OUTPUT_CSV} ذخیره شد.\")\n",
        "\n",
        "    # ایجاد گزارش برای بررسی خبره\n",
        "    generate_expert_report(processed_df, EXPERT_REPORT_CSV)\n",
        "    logging.info(f'✅ مرحله ۱ تکمیل شد! لطفاً فایل {EXPERT_REPORT_CSV} را به خبره ارسال کنید.')\n",
        "\n",
        "    # پس از دریافت فایل اصلاح‌شده توسط خبره:\n",
        "    # expert_reviewed = pd.read_csv('expert_reviewed.csv')\n",
        "    # kappa = calculate_irr(processed_df['benign'], expert_reviewed['expert_benign'])\n",
        "    # logging.info(f'میزان توافق (IRR): {kappa:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<div dir=\"rtl\">\n",
        "\n",
        "<h2> مراحل کار بخش دوم پروژه</h2>\n",
        "\n",
        "<h3>1️⃣ بارگذاری داده‌های برچسب‌خورده</h3>\n",
        "<ul>\n",
        "  <li>ورودی: فایل <code>labeled_threats.csv</code> (خروجی مرحله اول)</li>\n",
        "  <li>شامل اطلاعات لاگ‌ها و ستون‌های: <code>matched_capecs</code>، <code>matched_apts</code>، <code>benign</code>، <code>expert_validation</code></li>\n",
        "  <li>برچسب نهایی داده‌ها در ستون <code>final_label</code> قرار می‌گیرد</li>\n",
        "</ul>\n",
        "\n",
        "<h3>2️⃣ تقسیم داده‌ها به آموزش و تست</h3>\n",
        "<ul>\n",
        "  <li>با <code>train_test_split</code> داده‌ها به دو بخش آموزش (70٪) و تست (30٪) تقسیم می‌شوند</li>\n",
        "  <li>از <code>stratify</code> برای حفظ نسبت تهدید و عادی استفاده می‌شود</li>\n",
        "</ul>\n",
        "\n",
        "<h3>3️⃣ پیش‌پردازش ویژگی‌ها</h3>\n",
        "<ul>\n",
        "  <li><strong>ویژگی متنی:</strong> ستون <code>message</code> → پردازش با <code>TF-IDF</code> و n-gram</li>\n",
        "  <li><strong>ویژگی عددی:</strong> ستون‌های <code>status</code>، <code>bytes</code> → مقیاس‌بندی با <code>RobustScaler</code></li>\n",
        "  <li><strong>ویژگی دسته‌ای:</strong> ستون‌های <code>level</code>، <code>component</code>، <code>host</code> → کدگذاری با <code>OneHotEncoder</code></li>\n",
        "</ul>\n",
        "\n",
        "<h3>4️⃣ مدل‌سازی</h3>\n",
        "<ul>\n",
        "  <li>مدل: <code>RandomForestClassifier</code> (مدل نظارت شده)</li>\n",
        "  <li>تنظیم پارامترها با <code>GridSearchCV</code> برای انتخاب بهترین مقدار <code>n_estimators</code> و <code>max_depth</code></li>\n",
        "</ul>\n",
        "\n",
        "<h3>5️⃣ ارزیابی مدل</h3>\n",
        "<ul>\n",
        "  <li>ارزیابی روی مجموعه تست با متریک‌های:</li>\n",
        "  <ul>\n",
        "    <li><code>classification_report</code>: precision, recall, f1-score</li>\n",
        "    <li><code>confusion_matrix</code>: تعداد پیش‌بینی درست و غلط</li>\n",
        "    <li><strong>Recall برای تهدیدها</strong>: مهم‌ترین معیار</li>\n",
        "  </ul>\n",
        "</ul>\n",
        "\n",
        "<h3>6️⃣ گزارش تهدیدات پیشرفته (APT)</h3>\n",
        "<ul>\n",
        "  <li>بررسی لاگ‌های پیش‌بینی‌شده به‌عنوان تهدید (label=0)</li>\n",
        "  <li>تحلیل ستون <code>matched_capecs</code> و نگاشت آن‌ها به APT Groupها</li>\n",
        "  <li>نمایش نوع و تعداد تهدیدات</li>\n",
        "</ul>\n",
        "\n",
        "<h3>7️⃣ ذخیره مدل آموزش‌دیده</h3>\n",
        "<ul>\n",
        "  <li>مدل آموزش‌دیده در فایل <code>apt_aware_supervised.pkl</code> ذخیره می‌شود</li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<h2>🟢 ورودی‌ها</h2>\n",
        "<ul>\n",
        "  <li><code>labeled_threats.csv</code>: داده‌های لاگ پردازش شده و برچسب‌خورده</li>\n",
        "  <li><code>capec_apt_mapping.csv</code>: نگاشت CAPEC به APT Group</li>\n",
        "</ul>\n",
        "\n",
        "<h2>🔵 خروجی‌ها</h2>\n",
        "<ul>\n",
        "  <li>گزارش ارزیابی مدل (precision, recall, confusion matrix)</li>\n",
        "  <li>گزارش تهدیدات پیشرفته APT</li>\n",
        "  <li>فایل مدل ذخیره شده: <code>apt_aware_supervised.pkl</code></li>\n",
        "</ul>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<h2>📄 نحوه خواندن و تفسیر خروجی‌ها</h2>\n",
        "\n",
        "<h3>1️⃣ مدل ذخیره شده</h3>\n",
        "<p>مدل ذخیره شده با <code>joblib</code> بارگذاری می‌شود:</p>\n",
        "<pre><code>import joblib\n",
        "model = joblib.load('apt_aware_supervised.pkl')</code></pre>\n",
        "<p>سپس می‌توانید داده‌های جدید را مشابه قبل پردازش و پیش‌بینی کنید.</p>\n",
        "\n",
        "<h3>2️⃣ گزارش ارزیابی مدل</h3>\n",
        "<p>شامل:</p>\n",
        "<ul>\n",
        "  <li><strong>Support:</strong> تعداد واقعی نمونه‌های هر کلاس</li>\n",
        "  <li><strong>Precision:</strong> دقت پیش‌بینی‌ها</li>\n",
        "  <li><strong>Recall:</strong> مخصوصاً برای تهدیدها بسیار مهم است</li>\n",
        "  <li><strong>Confusion Matrix:</strong> نمایش پیش‌بینی درست و غلط</li>\n",
        "</ul>\n",
        "\n",
        "<h3>3️⃣ گزارش تهدیدات پیشرفته (APT)</h3>\n",
        "<p>نشان می‌دهد مدل چه نوع تهدیداتی (APT Groupها) را شناسایی کرده و چند مورد از هر نوع وجود داشته.</p>\n"
      ],
      "metadata": {
        "id": "Y53ThlaOQlAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, recall_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import joblib\n",
        "import logging\n",
        "\n",
        "# تنظیمات لاگینگ\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# ---------------------------\n",
        "def load_data(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"بارگذاری داده‌ها و ایجاد برچسب نهایی\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if 'expert_benign' in df.columns:\n",
        "        logging.info(\"✅ استفاده از برچسب‌های خبره\")\n",
        "        df['final_label'] = np.where(\n",
        "            df['expert_benign'].notna(),\n",
        "            df['expert_benign'].astype(int),\n",
        "            df['benign']\n",
        "        )\n",
        "    else:\n",
        "        logging.warning(\"⚠️ استفاده از برچسب‌های خودکار\")\n",
        "        df['final_label'] = df['benign']\n",
        "\n",
        "    logging.info(\"\\n📊 آمار کل داده:\")\n",
        "    logging.info(df['final_label'].value_counts())\n",
        "    return df\n",
        "\n",
        "# ---------------------------\n",
        "def get_preprocessor():\n",
        "    \"\"\"\n",
        "    پیش‌پردازش ویژگی‌ها شامل بخش متنی (با TF-IDF) و عددی/دسته‌ای.\n",
        "    \"\"\"\n",
        "    text_processor = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(\n",
        "            max_features=3000,\n",
        "            ngram_range=(1, 2),\n",
        "            stop_words='english',\n",
        "            sublinear_tf=True,\n",
        "            analyzer='word'\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('text', text_processor, 'message'),\n",
        "            ('num', RobustScaler(), ['status', 'bytes']),\n",
        "            ('cat', OneHotEncoder(\n",
        "                handle_unknown='infrequent_if_exist',\n",
        "                sparse_output=False,\n",
        "                min_frequency=10\n",
        "            ), ['level', 'component', 'host'])\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "# ---------------------------\n",
        "def train_model_supervised(X_train: pd.DataFrame, y_train: pd.Series) -> Pipeline:\n",
        "    \"\"\"آموزش مدل نظارت شده با استفاده از GridSearchCV برای تنظیم پارامترها\"\"\"\n",
        "    preprocessor = get_preprocessor()\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    param_grid = {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [None, 10, 20],\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring='f1_weighted',\n",
        "        verbose=1,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    logging.info(\"\\n🚀 شروع آموزش مدل نظارت شده با GridSearchCV:\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    logging.info(f\"بهترین پارامترها: {grid_search.best_params_}\")\n",
        "\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# ---------------------------\n",
        "def analyze_apts(threat_data: pd.DataFrame, capec_apt_map: pd.DataFrame) -> dict:\n",
        "    \"\"\"تحلیل APT با استفاده از نگاشت CAPEC به APT\"\"\"\n",
        "    apt_counts = {}\n",
        "    for _, row in threat_data.iterrows():\n",
        "        if pd.isna(row.get('matched_capecs')):\n",
        "            continue\n",
        "        capecs = [c.strip() for c in row['matched_capecs'].split(',')]\n",
        "        for capec in capecs:\n",
        "            apts = capec_apt_map[capec_apt_map['CAPEC-ID'] == capec]['APT Group']\n",
        "            for apt in apts:\n",
        "                apt_counts[apt] = apt_counts.get(apt, 0) + 1\n",
        "    return dict(sorted(apt_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# ---------------------------\n",
        "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: pd.Series, capec_apt_map: pd.DataFrame):\n",
        "    \"\"\"ارزیابی مدل نظارت شده و چاپ گزارش عملکرد\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"\\n📊 آمار داده‌های تست:\")\n",
        "    print(y_test.value_counts())\n",
        "\n",
        "    report = classification_report(\n",
        "        y_test,\n",
        "        y_pred,\n",
        "        target_names=['تهدید (0)', 'عادی (1)'],\n",
        "        digits=4\n",
        "    )\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    threat_recall = recall_score(y_test, y_pred, pos_label=0)\n",
        "\n",
        "    print(\"\\n📊 گزارش عملکرد:\")\n",
        "    print(report)\n",
        "    print(\"\\n🎯 ماتریس درهم‌ریختگی:\")\n",
        "    print(cm)\n",
        "    print(f\"\\n📈 Recall برای تهدیدها: {threat_recall:.2%}\")\n",
        "\n",
        "    # گزارش APT (در صورتی که ستون matched_capecs موجود باشد)\n",
        "    if 'matched_capecs' in X_test.columns:\n",
        "        X_test = X_test.copy()\n",
        "        X_test['predicted_label'] = y_pred\n",
        "        threat_logs = X_test[X_test['predicted_label'] == 0]\n",
        "        print(\"\\n🔍 گزارش تهدیدات پیشرفته (APT) برای لاگ‌های خطرناک:\")\n",
        "        if not threat_logs.empty:\n",
        "            apt_report = analyze_apts(threat_logs, capec_apt_map)\n",
        "            for apt, count in apt_report.items():\n",
        "                print(f\"- {apt}: {count} مورد\")\n",
        "        else:\n",
        "            print(\"⚠️ هیچ تهدیدی شناسایی نشد!\")\n",
        "    else:\n",
        "        print(\"ستون matched_capecs در داده‌ها موجود نیست؛ بنابراین گزارش APT تولید نمی‌شود.\")\n",
        "\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    INPUT_DATA = \"labeled_threats.csv\"\n",
        "    CAPEC_APT_MAP = \"capec_apt_mapping.csv\"\n",
        "\n",
        "    # بارگذاری داده‌ها\n",
        "    df = load_data(INPUT_DATA)\n",
        "\n",
        "    # تقسیم داده‌ها به آموزش و تست (70/30)\n",
        "    X = df.drop(columns=['final_label'])\n",
        "    y = df['final_label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(\"\\n📊 آمار داده‌های آموزش:\")\n",
        "    print(y_train.value_counts())\n",
        "    print(\"➖➖➖\")\n",
        "    print(\"📊 آمار داده‌های تست:\")\n",
        "    print(y_test.value_counts())\n",
        "    print(\"➖➖➖\")\n",
        "\n",
        "    capec_apt_map = pd.read_csv(CAPEC_APT_MAP)\n",
        "\n",
        "    # آموزش مدل\n",
        "    model = train_model_supervised(X_train, y_train)\n",
        "\n",
        "    # ارزیابی مدل\n",
        "    evaluate_model(model, X_test, y_test, capec_apt_map)\n",
        "\n",
        "    # ذخیره مدل\n",
        "    joblib.dump(model, 'apt_aware_supervised.pkl')\n",
        "    print(\"\\n💾 مدل ذخیره شد.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYk74VtxHp9d",
        "outputId": "cee921f0-d880-4334-a33d-2000fb60cfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:⚠️ استفاده از برچسب‌های خودکار\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 آمار داده‌های آموزش:\n",
            "final_label\n",
            "1    85\n",
            "0    61\n",
            "Name: count, dtype: int64\n",
            "➖➖➖\n",
            "📊 آمار داده‌های تست:\n",
            "final_label\n",
            "1    36\n",
            "0    27\n",
            "Name: count, dtype: int64\n",
            "➖➖➖\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "\n",
            "📊 آمار داده‌های تست:\n",
            "final_label\n",
            "1    36\n",
            "0    27\n",
            "Name: count, dtype: int64\n",
            "\n",
            "📊 گزارش عملکرد:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   تهدید (0)     1.0000    1.0000    1.0000        27\n",
            "    عادی (1)     1.0000    1.0000    1.0000        36\n",
            "\n",
            "    accuracy                         1.0000        63\n",
            "   macro avg     1.0000    1.0000    1.0000        63\n",
            "weighted avg     1.0000    1.0000    1.0000        63\n",
            "\n",
            "\n",
            "🎯 ماتریس درهم‌ریختگی:\n",
            "[[27  0]\n",
            " [ 0 36]]\n",
            "\n",
            "📈 Recall برای تهدیدها: 100.00%\n",
            "\n",
            "🔍 گزارش تهدیدات پیشرفته (APT) برای لاگ‌های خطرناک:\n",
            "- China-based APT groups: 2 مورد\n",
            "- Chimera, APT-C-23, APT3: 1 مورد\n",
            "\n",
            "💾 مدل ذخیره شد.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<div dir=\"rtl\">\n",
        "<h2> مدل تمام تهدیدات رو درست پیش بینی کرده که به دلایل زیر هست:</h2>\n",
        "تعداد کم داده‌ها و یکنواخت بودن:\n",
        "فقط 209 لاگ داریم.\n",
        "داده‌ها خیلی متنوع نیستن و الگوی خاصی به وضوح بین تهدید و عادی وجود داره.\n",
        "\n",
        "همچنین با stratify split نسبت داده‌های تهدید و عادی در آموزش و تست حفظ شده و تفاوت‌ها خیلی واضح هستند.\n",
        "\n",
        "4️⃣ مدل RandomForest قدرت بالایی در جدا کردن داده‌های ساده داره:\n",
        "RandomForest مدل قدرتمندیه، وقتی داده‌ها الگوهای واضح و بدون نویز داشته باشن، به راحتی می‌تونه بدون خطا پیش‌بینی کنه.\n",
        "\n"
      ],
      "metadata": {
        "id": "9tFAsZYTRLSM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbgsYx0TMs-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}